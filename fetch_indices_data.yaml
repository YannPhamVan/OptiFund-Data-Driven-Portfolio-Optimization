id: fetch_indices_data
namespace: zoomcamp-project

tasks:
  - id: fetch_data
    type: io.kestra.plugin.scripts.python.Script
    description: "Fetch historical index data using yfinance and load into GCS"
    docker:
      image: python:3.9-slim
    beforeCommands:
      - pip install --no-cache-dir yfinance pandas
    script: |
      import yfinance as yf
      import pandas as pd

      # Define all 38 indices
      tickers = [
          "^AEX", "^AORD", "^BSESN", "^BVSP", "^CN20", "^GSPC", "^IBEX", "^IXIC",
          "^N100", "^N150", "^NBI", "^NDX", "^OEX", "^RUT", "^SKEW", "^SOX",
          "^SPXEW", "000001.SS", "^STOXX", "^STOXX50E", "^VIX", "^VVIX", "^VXD",
          "^VXN", "^W5000", "^AXJO", "^DJI", "^DJT", "^DJU", "^FCHI", "^FTSE",
          "^GDAXI", "^HSI", "^N225", "^NZ50", "^OMXC25", "^OSEAX", "^STI"
      ]

      # Fetch data in one request
      df = yf.download(tickers, period="9d", interval="1d", auto_adjust=True)["Close"]
      # Save as CSV
      file_path = "data.csv"
      df.to_csv(file_path, index=True)

    outputFiles:
      - data.csv  # Explicitly declare the output file

  - id: upload_to_gcs
    type: io.kestra.plugin.gcp.gcs.Upload
    description: "Upload the extracted data to GCS"
    serviceAccount: "{{kv('GCP_CREDS')}}"
    projectId: "{{kv('GCP_PROJECT_ID')}}"
    from: "{{ outputs.fetch_data.outputFiles['data.csv'] }}"
    to: "gs://{{kv('GCP_BUCKET_NAME')}}/raw/data.csv"

  - id: load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Load data from GCS to BigQuery using dlt"
    docker:
      image: python:3.9-slim
    beforeCommands:
      - pip install --no-cache-dir dlt[bigquery] pandas
    script: |
      import dlt
      import pandas as pd
      import gcsfs

      # Initialize dlt pipeline
      pipeline = dlt.pipeline(
          pipeline_name="gcs_to_bigquery",
          destination="bigquery",
          dataset_name="{{kv('GCP_DATASET')}}"
      )

      # Read CSV from GCS
      fs = gcsfs.GCSFileSystem()
      gcs_path = "gs://{{kv('GCP_BUCKET_NAME')}}/raw/data.csv"
      with fs.open(gcs_path, 'r') as f:
        df = pd.read_csv(f)

      # Load to BigQuery
      pipeline.run(df, table_name="indices_data", write_disposition="replace")

  - id: purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: If you'd like to explore Kestra outputs, disable it.
    disabled: true

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDS')}}"
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_NAME')}}"

triggers:
  - id: daily_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 0 * * *"  # Daily execution at 00:00 UTC