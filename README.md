# OptiFund: Data-Driven Portfolio Optimization

## ğŸ§  Project Overview

**OptiFund** is a data engineering project aiming to build an optimized portfolio from the main world indexes.  
Using historical daily data, the pipeline identifies top-performing assets and removes highly correlated ones to maximize diversification and return.

## ğŸ”§ Tech Stack

- **Kestra** for orchestration
- **Google Cloud Platform** (GCS, BigQuery)
- **Terraform** for infrastructure as code
- **dbt** for analytics transformations
- **Looker Studio** for dashboard visualization
- **yfinance** for data ingestion

## ğŸ“ Project Structure

```bash
â”œâ”€â”€ notebooks/              # Testing Data ingestion using yfinance
â”œâ”€â”€ terraform/              # Infrastructure setup on GCP
â”œâ”€â”€ orchestration_kestra/   # Kestra flows for orchestration
â”œâ”€â”€ dbt_optifund/           # dbt transformations and models
â””â”€â”€ README.md               # Project documentation
```

## ğŸ“Š Key Results

- Correlation matrix of top assets
- Portfolio construction excluding overlapping assets
- Interactive dashboard with filters by return/correlation

## ğŸš€ Try It

View the dashboard on Looker Studio ([link here](https://lookerstudio.google.com/reporting/3c99e91f-961a-4504-8187-91c88275a8d5))

## ğŸ“Œ Status

âœ… Ingestion
âœ… Orchestration
âœ… BigQuery pipeline
ğŸŸ¡ Correlation-based selection (enhancing the sort by return)
ğŸŸ¢ Final dashboard in progress

## ğŸš€ Reproducibility

This project has been designed to be easily reproducible by following the steps below.

### 1. Prerequisites

- Google Cloud account with billing enabled
![Industrial Equipment Monitoring](images/Setting up a service account on GCP.png)
- Enabled APIs:
  - BigQuery
  - Cloud Storage
  - Service Account
- A project created in GCP (this project uses the `EU` multi-region)
- Requirements :
  - Python 3.10+
  - `pipenv` installed globally
  ```bash
  pip install pipenv
  pipenv shell
  ```

### 2. Clone the repository

```bash
git clone https://github.com/YannPhamVan/OptiFund-Data-Driven-Portfolio-Optimization.git
cd OptiFund-Data-Driven-Portfolio-Optimization
```

### 3. Infrastructure setup (Terraform)

Go to the terraform folder and apply the infrastructure:
```bash
cd terraform
terraform init
terraform apply
```
This creates:

- A Cloud Storage bucket for raw data
- A BigQuery dataset for the analytics layer
- A service account (key not committed)

âš ï¸ Create a file named `keys/my-creds.json` in the terraform folder with your own GCP credentials.

### 4. Data ingestion (Kestra)

The ingestion pipeline is orchestrated with [Kestra](https://kestra.io/).
You can view the flow in `orchestration_kestra/flows/` folder.

To start Kestra:
```bash
cd ../orchestration_kestra
docker compose up -d
```
Then open your browser at: http://localhost:8080

To stop Kestra:
```bash
docker compose down
```
**â„¹ï¸ Kestra workflows are located in the `orchestration_kestra/flows/` folder. These manage the ingestion and loading of stock data into GCS and BigQuery.**

Once Kestra is installed and running locally (via Docker), launch the flow directly from the UI or using the CLI.

The flow:
- Calls `yfinance` to collect daily closing prices of indexes
- Saves the files in Cloud Storage
- Loads the data into BigQuery

### 5. Data transformation (dbt)

Navigate to the `dbt_optifund` folder, then run the dbt pipeline:
```bash
cd ../dbt_optifund
dbt debug
dbt deps
dbt run --select cumulative_returns
dbt run --select correlation_matrix
```
This will transform the raw data, compute returns and correlations, and prepare the final models for analysis.

### 6. Dashboard (Looker Studio)

A Looker Studio dashboard is available [here](https://lookerstudio.google.com/reporting/3c99e91f-961a-4504-8187-91c88275a8d5)

It includes:
- An overview of the best-performing stocks
- A correlation matrix
- Portfolio composition visuals

Feel free to adapt paths or commands depending on your setup.